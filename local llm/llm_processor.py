from langchain_ollama import OllamaLLM
from langchain_core.prompts import ChatPromptTemplate

class LocalLLMProcessor:
    def __init__(self, model_name="llama3"):
        """
        Initialize the LLM processor with a specific local model.
        Ensure you have Ollama installed and the model pulled: `ollama pull <model_name>`
        """
        self.llm = OllamaLLM(model=model_name)

    def enhance_text(self, ocr_text):
        """
        Takes raw OCR text and uses the LLM to clean and correct it.
        """
        prompt = ChatPromptTemplate.from_template(
            """
            You are an expert editor. Your task is to correct the following text generated by an OCR system.
            The text may contain typos, misread characters, or formatting issues.
            Please output ONLY the corrected text. Do not add any conversational filler.

            Raw OCR Text:
            {text}
            """
        )
        chain = prompt | self.llm
        try:
            result = chain.invoke({"text": ocr_text})
            return result
        except Exception as e:
            return f"Error processing text: {str(e)}"

    def query_text(self, context_text, query):
        """
        Allows the user to ask questions based on the provided context text.
        """
        prompt = ChatPromptTemplate.from_template(
            """
            Answer the question strictly based on the following context.
            
            Context:
            {context}

            Question:
            {question}
            
            Answer:
            """
        )
        chain = prompt | self.llm
        try:
            result = chain.invoke({"context": context_text, "question": query})
            return result
        except Exception as e:
            return f"Error answering query: {str(e)}"
